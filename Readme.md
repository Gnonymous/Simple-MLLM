# Simple-MLLM
> Simple-MLLM is a simple locally deployed Multimodal Large Model (MLLM) practice. Let's get your hands on the magic of MLLM, just on your local machine!

ğŸ˜è¿™æ˜¯ä¸€ä¸ªåœ¨æœ¬åœ°éƒ¨ç½²çš„**ç®€æ˜“**å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆMLLMï¼‰çš„å®ä¾‹ï¼Œæ”¯æŒåŒ…æ‹¬å›¾ç‰‡ã€æ–‡å­—ä»¥åŠè¯­éŸ³ï¼ˆ*æ­£åœ¨æ›´æ–°*ï¼‰å¤šç§æ¨¡æ€çš„è¾“å…¥ã€‚

ğŸ¤©å¦‚æœä½ å¯¹å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆMLLMï¼‰æ„Ÿå…´è¶£ä½†æ˜¯ä¸çŸ¥å¦‚ä½•ä¸Šæ‰‹ï¼Œæ¥çœ‹çœ‹è¿™ä¸ªé¡¹ç›®ï¼šä¸€ä¸ªç®€æ˜“çš„é¡¹ç›®è®©ä½ æ„Ÿå—åˆ°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„é­”åŠ›ğŸ‘

---
ğŸ˜This is a **simple** example of deploying Multimodal Large Model (MLLM) locally with support for multiple modal inputs including image, text and voice (*being updated*).

ğŸ¤©If you are interested in Multimodal Large Models (MLLM) but don't know how to get started. Take a look at this project: a simple project that will give you a taste of the magic of MLLM ğŸ‘

## Overview
The input is img as below:

![alt img](Introduce/frame.jpg)

And then I ask the model:"æè¿°â€, after that the checkpoints are loaded. Finally:

The output is description as below:

![alt img](Introduce/demo.jpg)
## Based Structureï¼š

* Socket
* Qwen2-VL-3B
* Whisperï¼ˆ*being updated*ï¼‰